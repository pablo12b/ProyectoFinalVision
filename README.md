# Proyecto Final: Sistema de Vigilancia Inteligente con HOG y YOLO

Este proyecto implementa un sistema de vigilancia h칤brido que combina la eficiencia de **C++ con HOG+SVM** para la detecci칩n en tiempo real y la precisi칩n de **Python con YOLOv11** para el an치lisis de posturas y notificaciones.

## 游늭 Archivos del Proyecto
*   **`generar_dataset.py`**: Script para crear y aumentar el dataset de entrenamiento para HOG.
*   **`main.cpp`**: Aplicaci칩n cliente en C++ que captura video, detecta personas y env칤a alertas.
*   **`servidor.py`**: Servidor Flask en Python que recibe alertas, valida con YOLO y notifica por Telegram.

---

## 1. Dataset y Generaci칩n de Datos
Para mejorar la detecci칩n de personas mediante HOG, se realiz칩 un proceso exhaustivo de curaci칩n y aumento de datos.

### Origen de Datos
Se utiliz칩 un dataset base de estimaci칩n de posturas (`pose-estimation.v3i.yolov8`). Este formato contiene im치genes completas y archivos de etiquetas que indican la ubicaci칩n de las personas.

### Preprocesamiento (`generar_dataset.py`)
El script de generaci칩n realiza los siguientes pasos para crear muestras positivas de alta calidad:
1.  **Lectura y Parseo**: Lee las im치genes y sus correspondientes etiquetas YOLO (.txt).
2.  **Extracci칩n (Cropping)**: Utiliza las coordenadas de las cajas delimitadoras (bounding boxes) para recortar 칰nicamente a las personas de la escena.
3.  **Normalizaci칩n**: Cada recorte se redimensiona a **64x128 p칤xeles**, que es el tama침o est치ndar requerido por el descriptor HOG de OpenCV.
4.  **Validaci칩n**: Se descartan recortes demasiado peque침os o mal formados para evitar ruido en el entrenamiento.

### Data Augmentation (Aumento de Datos)
Para cumplir con los requisitos de robustez y simular condiciones reales (como el movimiento de una c치mara o desenfoque), se implement칩 un pipeline de aumentaci칩n utilizando la librer칤a **Albumentations**. Por cada persona detectada, se generan **15 variaciones** incluyendo:
*   **Horizontal Flip**: Efecto espejo (p=0.5).
*   **Motion Blur**: Simulaci칩n de desenfoque por movimiento (clave para video en tiempo real).
*   **Rotaci칩n**: Leves rotaciones (췀10 grados) para simular posturas naturales.
*   **Ruido Gaussiano**: Para robustecer el detector ante c치maras con 'grano' o baja luz.
*   **Brillo y Contraste**: Variaciones de iluminaci칩n.

**Resultado:** Un dataset robusto almacenado en `dataset_hog/pos` listo para entrenar o validar detectores HOG.

---

## 2. Documentaci칩n T칠cnica de Implementaci칩n

### 2.1. Cliente C++ (`main.cpp`) - Detector Local
Este m칩dulo act칰a como un agente de borde (Edge Agent), optimizado para bajo consumo de recursos y alta velocidad de respuesta.

#### Arquitectura y Librer칤as
*   **Core**: Utiliza `opencv2/objdetect.hpp` para el descriptor HOG y `opencv2/imgproc.hpp` para operaciones matriciales.
*   **Comunicaci칩n**: Invoca llamadas al sistema (`system()`) para ejecutar `curl`, permitiendo el env칤o as칤ncrono de datos (con `&`) para no bloquear el hilo de captura de video.

#### L칩gica de Detecci칩n (HOG + SVM)
Se instancia un `HOGDescriptor` configurado con el **detector de personas por defecto (INRIA)** (`HOGDescriptor::getDefaultPeopleDetector()`). Esto carga los coeficientes del hiperplano de soporte de un SVM lineal pre-entrenado.

**Par치metros Cr칤ticos de `detectMultiScale`:**
*   **`hitThreshold` (0.3)**: Define el margen de tolerancia para la clasificaci칩n del SVM. Un valor m치s bajo aumenta el recall (detecta m치s) a costo de precisi칩n. Se ajust칩 a 0.3 para maximizar la sensibilidad.
*   **`winStride` (8,8)**: El paso de la ventana deslizante. Un paso de 8px (mitad de celda) ofrece un equilibrio 칩ptimo entre cobertura y coste computacional.
*   **`scale` (1.05)**: Factor de escalado para la pir치mide de im치genes. El algoritmo reduce la imagen un 5% en cada nivel para detectar personas a diferentes distancias.
*   **`groupThreshold` (2)**: Requiere que al menos 2 rect치ngulos detectados se superpongan para considerar una detecci칩n v치lida, eliminando ruido espor치dico.

#### Heur칤stica de Filtrado Geom칠trico
Para reducir falsos positivos que el HOG pueda dejar pasar, se implementan filtros post-detecci칩n basados en la geometr칤a esperada de un humano:
1.  **Filtro de Altura**: `60px < h < 470px`. Descarta objetos demasiado peque침os (lejos/ruido) o que ocupan toda la pantalla.
2.  **Ratio de Aspecto (Aspect Ratio)**: Se calcula `ratio = width / height`. Se aceptan solo detecciones con `0.2 < ratio < 0.85`, descartando objetos muy anchos (coches, muebles) o extremadamente delgados.
3.  **Supresi칩n de Bordes**: Se ignoran detecciones que tocan los bordes del frame (`x < 2`), ya que los descriptores HOG incompletos suelen generar falsos positivos.

#### M치quina de Estados (Detecci칩n Temporal)
*   **Temporal Consistency**: Se implementa un contador (`contadorDeteccion`). Se requiere que el flujo HOG detecte una persona en **3 frames consecutivos procesados** (aprox. 0.5s reales) antes de activar una alerta.
*   **Cooldown**: Tras una alerta, el sistema entra en un estado de "enfriamiento" por 40 ciclos de loop, evitando ataques de denegaci칩n de servicio (DoS) hacia el servidor.

---

### 2.2. Servidor Python (`servidor.py`) - Inferencia de Alto Nivel
Implementado como un microservicio RESTful modular utilizando **Flask**, encargado de la validaci칩n sem치ntica y la respuesta.

#### Pipeline de Procesamiento de Imagen
1.  **Ingesta en Memoria**:
    El endpoint `/detectar` recibe el archivo mediante `request.files`.
    *   *T칠cnica*: No se guarda en disco inmediatamente. Se lee el stream de bytes (`file.read()`) y se convierte a un buffer numpy (`np.frombuffer`), decodific치ndolo finalmente con `cv2.imdecode`. Esto reduce la latencia de I/O dr치sticamente.

#### Motor de Inferencia (Ultralytics YOLOv11)
Se utiliza el modelo `yolo11n-pose.pt` (Nano), cuantizado para inferencia r치pida en CPU.
*   **Tarea**: Pose Estimation (Keypoint Detection).
*   **Inferencia**: `model.predict(img, conf=0.5)`. Se establece un umbral de confianza del 50%.
*   **Salida**: El modelo retorna un objeto `Results` que contiene:
    *   `boxes`: Bounding boxes de las personas.
    *   `keypoints`: Coordenadas (x,y) de 17 articulaciones (hombros, codos, rodillas, etc.).

#### Generaci칩n de Evidencia y Notificaci칩n
1.  **Visualizaci칩n**: Se utiliza el m칠todo `.plot()` nativo de Ultralytics para renderizar el esqueleto sobre la imagen original.
2.  **Persistencia**: Se guardan dos versiones de la evidencia con timestamps precisos:
    *   `_org.jpg`: La imagen cruda enviada por el HOG (para auditor칤a).
    *   `_proc.jpg`: La imagen con el esqueleto superpuesto (para validaci칩n visual).
3.  **Integraci칩n con Telegram**:
    Se utiliza la librer칤a `telebot` (pyTelegramBotAPI) en modo s칤ncrono. Se env칤a la imagen procesada con un *caption* formateado como alerta de seguridad. El env칤o est치 encapsulado en un bloque `try-except` para garantizar que un fallo de red en la API de Telegram no tumbe el servicio de inferencia.

---

## 游 C칩mo Ejecutar

1.  **Iniciar el Servidor (Cerebro):**
    ```bash
    python3 servidor.py
    ```
    *Aseg칰rate de tener configurado tu TOKEN y CHAT_ID de Telegram en el script.*

2.  **Iniciar el Cliente (Ojos):**
    ```bash
    # Compilar (aseg칰rate de tener CMake y OpenCV instalados)
    cmake .
    make
    # Ejecutar
    ./ProyectoFinal
    ```
